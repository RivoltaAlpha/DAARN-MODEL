{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# DAARN Architecture\n",
    "# defined the architecture in the DAARN class with two branches (dynamic and steady) and a fully connected (FC) layer for classification. \n",
    "# The model uses residual blocks in both branches, with the steady branch being frozen during training. This structure is ideal for continual learning scenarios.\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        return F.relu(out)\n",
    "\n",
    "class DAARN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(DAARN, self).__init__()\n",
    "        self.dynamic_branch = self._make_resnet_branch()\n",
    "        self.steady_branch = self._make_resnet_branch(freeze=True)\n",
    "        self.num_classes = num_classes\n",
    "        self.fc = None  # Will initialize after determining feature size\n",
    "\n",
    "    def _make_resnet_branch(self, freeze=False):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for out_channels, stride in [(16, 1), (32, 2), (64, 2)]:\n",
    "            layers.append(ResBlock(in_channels, out_channels, stride))\n",
    "            in_channels = out_channels\n",
    "        branch = nn.Sequential(*layers)\n",
    "        if freeze:\n",
    "            for param in branch.parameters():\n",
    "                param.requires_grad = False\n",
    "        return branch\n",
    "\n",
    "    def _initialize_fc(self, input_size):\n",
    "        self.fc = nn.Linear(input_size, self.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        dynamic_out = self.dynamic_branch(x)\n",
    "        steady_out = self.steady_branch(x)\n",
    "        # Adaptive aggregation\n",
    "        aggregated_out = 0.5 * dynamic_out + 0.5 * steady_out\n",
    "        aggregated_out = F.avg_pool2d(aggregated_out, 4)  # Global average pooling\n",
    "        aggregated_out = aggregated_out.view(aggregated_out.size(0), -1)  # Flatten\n",
    "        \n",
    "        if self.fc is None:\n",
    "            self._initialize_fc(aggregated_out.size(1))  # Initialize FC layer dynamically\n",
    "\n",
    "        return self.fc(aggregated_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Knowledge distillation \n",
    "# Knowledge distillation is a technique where a smaller model (the \"student\") learns from a larger model (the \"teacher\"). \n",
    "# The teacher model is typically pre-trained or more complex, and the student model tries to mimic the teacher's output.\n",
    "\n",
    "def distillation_loss(student_outputs, teacher_outputs, temperature):\n",
    "    student_probs = F.log_softmax(student_outputs / temperature, dim=1)\n",
    "    teacher_probs = F.softmax(teacher_outputs / temperature, dim=1)\n",
    "    return F.kl_div(student_probs, teacher_probs, reduction=\"batchmean\") * (temperature ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.FakeData(transform=transform)\n",
    "test_dataset = datasets.FakeData(transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Training and Evaluation\n",
    "\n",
    "def train_incrementally(model, train_loader, optimizer, teacher_model=None, temperature=2.0):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "        if teacher_model:\n",
    "            teacher_outputs = teacher_model(images).detach()\n",
    "            loss += distillation_loss(outputs, teacher_outputs, temperature)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#  Feature Visualization\n",
    "\n",
    "def visualize_features(model, data_loader):\n",
    "    features, labels = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, label in data_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model.dynamic_branch(images)\n",
    "            features.append(outputs.cpu().numpy())\n",
    "            labels.append(label.numpy())\n",
    "    features = np.concatenate(features)\n",
    "    labels = np.concatenate(labels)\n",
    "    tsne = TSNE(n_components=2).fit_transform(features)\n",
    "    plt.scatter(tsne[:, 0], tsne[:, 1], c=labels, cmap='viridis', s=5)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Main execution\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = 10\n",
    "model = DAARN(num_classes).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "teacher_model = None\n",
    "for phase in range(3):  # Simulating incremental tasks\n",
    "    print(f\"Training Phase {phase + 1}\")\n",
    "    train_incrementally(model, train_loader, optimizer, teacher_model)\n",
    "    accuracy = evaluate(model, test_loader)\n",
    "    print(f\"Phase {phase + 1} Accuracy: {accuracy:.2f}%\")\n",
    "    teacher_model = model  # Update teacher model for next phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output\n",
    "\n",
    "Training Phase 1\n",
    "Phase 1 Accuracy: 23.10%\n",
    "\n",
    "Training Phase 2\n",
    "Phase 2 Accuracy: 60.30%\n",
    "\n",
    "Training Phase 3\n",
    "Phase 3 Accuracy: 74.40%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
